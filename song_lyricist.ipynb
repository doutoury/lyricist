{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<br>\n",
    "\n",
    "## 데이터 읽어오기\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "데이터 크기: 187088\nExamples:\n ['How does a bastard, orphan, son of a whore', 'And a Scotsman, dropped in the middle of a forgotten spot in the Caribbean by providence impoverished,', 'In squalor, grow up to be a hero and a scholar? The ten-dollar founding father without a father']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "glob 모듈 사용하여 모든 txt 파일 읽어와\n",
    "raw_corpus 리스트에 문장 단위로 저장\n",
    "\"\"\"\n",
    "\n",
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 리스트에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 데이터 전처리 : 필터링\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "데이터 전처리\n",
    "필터링 ( 원하는 학습에 적합하도록 데이터 정제 )\n",
    "1. 소문자화\n",
    "2. 특수문자 제거\n",
    "3. RNN 학습 위한 <start> <end> 추가\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    # 1. 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    # 2. 아래 3단계를 거쳐 sentence를 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바꿈\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)      # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)    # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 3. RNN 학습에 용이하도록 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    sentence = '<start> ' + sentence + ' <end>'     \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "156013 \n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<start> how does a bastard , orphan , son of a whore <end>',\n",
       " '<start> got a lot farther by working a lot harder <end>',\n",
       " '<start> by being a lot smarter by being a self starter <end>',\n",
       " '<start> across the waves , he struggled and kept his guard up <end>',\n",
       " '<start> inside , he was longing for something to be a part of <end>',\n",
       " '<start> our man saw his future drip , dripping down the drain <end>',\n",
       " '<start> put a pencil to his temple , connected it to his brain <end>',\n",
       " '<start> took up a collection just to send him to the mainland <end>',\n",
       " '<start> get your education , don t forget from whence you came <end>',\n",
       " '<start> and the world is gonna know your name <end>',\n",
       " '<start> what s your name , man ? alexander hamilton <end>',\n",
       " '<start> my name is alexander hamilton <end>',\n",
       " '<start> and there s a million things i haven t done <end>',\n",
       " '<start> two years later , see alex and his mother bed ridden <end>',\n",
       " '<start> left him with nothin but ruined pride , something new inside voice saying <end>',\n",
       " '<start> he woulda been dead or destitute without a cent of restitution <end>',\n",
       " '<start> started workin , clerkin for his late mother s landlord <end>',\n",
       " '<start> tradin sugar cane and rum and all the things he can t afford <end>',\n",
       " '<start> scammin for every book he can get his hands on <end>',\n",
       " '<start> will they know what you overcame ? <end>']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "\"\"\"\n",
    "데이터 전처리를 끝낸 말뭉치(corpus) 재생성\n",
    "\"\"\"\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue     # 공백 문장(줄) 제외\n",
    "    if sentence[-1] == \":\": continue    # : 로 끝나는 문장 제외\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 공백 개수로 토큰이 될 어휘 개수가 15개 넘어가는 문장 제외\n",
    "    if preprocessed_sentence.count(\" \") > 14: continue     \n",
    "\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "print(len(corpus), \"\\n\")\n",
    "corpus[:20]"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 데이터 전처리 : 토큰화 (Tokenize)와 벡터화(Vectorize) <br><br>\n",
    "\n",
    "__단어사전 {단어:숫자}__ <br>\n",
    "컴퓨터는 숫자로 의사소통을 합니다. 우리가 쓰는 언어(단어)를 숫자로 변환하여 컴퓨터에게 넘겨주어야 합니다. <br><br>\n",
    "\n",
    "이를 위해 {단어:숫자} 형태의 __단어사전__이 필요합니다. <br>\n",
    "단어사전은 __토큰화(tokenize)__를 통해 만듭니다. <br>\n",
    "이 때, 어휘(단어)를 숫자로 변환시켜주는 __벡터화(vectorize)__과정이 필요합니다. <br>\n",
    "이렇게 숫자로 변환된 데이터를 __텐서(tensor)__라고 합니다. <br><br>\n",
    "\n",
    "__tf.keras.preprocessing.text.Tokenizer 패키지__ <br>\n",
    "데이터의 토큰화, 벡터화를 한번에 해주는 tensorflow.keras의 전처리 라이브러리 입니다. <br>\n",
    "정제된 데이터를 토큰화하고, 단어 사전을 만들어주며, <br>\n",
    "데이터를 숫자로 변환하는 벡터화를 통해 텐서를 생성합니다.\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  2  79 622 ...   3   0   0]\n [  2  37   9 ...   0   0   0]\n [  2 115 573 ...   0   0   0]\n ...\n [  2   6 225 ...   0   0   0]\n [  2   4  61 ...   0   0   0]\n [  2   6 225 ...   0   0   0]] \n\n <keras_preprocessing.text.Tokenizer object at 0x7f6aea6e3650>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "데이터 전처리\n",
    "tf.keras.preprocessing.text.Tokenizer 패키지를 통한 토큰화(tokenize), 벡터화(vetorize)\n",
    "\n",
    "토큰화 : preprocessing.text.Tokenizer.fit_on_texts()\n",
    "벡터화 : preprocessing.text.Tokenizer.texts_to_sequences()\n",
    "패딩 : preprocessing.sequence.pad_sequences()\n",
    "\"\"\"\n",
    "\n",
    "def tokenize(corpus):\n",
    "\n",
    "    # 텐서플로우의 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,     # 단어장의 크기 (전체 단어의 개수) \n",
    "        filters=' ',        # 별도로 전처리 로직을 추가할 수 있습니다. (이번에는 사용하지 않겠습니다)\n",
    "        oov_token=\"<unk>\"   # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "\n",
    "    # 위에서 정제한 corpus에 대해 Tokenizer.fit_on_texts()를 통해 단어사전 생성\n",
    "    tokenizer.fit_on_texts(corpus)   \n",
    "\n",
    "    # Tokenizer.texts_to_sequences()를 통해 모델에 입력할 벡터(tensor) 데이터셋을 생성\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # 위에서 생성한 단어사전으로부터 corpus를 해석해 Tensor로 변환\n",
    "\n",
    "    # padding 메소드 : 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위해 패딩\n",
    "    # maxlen의 디폴트값은 None : 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor, \"\\n\\n\", tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\n(156013, 15)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 과정에서 토큰 개수가 15개 맞는지 확인\n",
    "# tensor 변수의 데이터 형태 확인\n",
    "print(len(tensor[0]))\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[   2   79  622    9 4486    5    1    5  646   20]\n [   2   37    9  441 3069  115 1141    9  441 1046]\n [   2  115  573    9  441 6826  115  573    9 1163]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "생성된 텐서 데이터 확인 (3번째 행, 10번째까지만 출력)\n",
    "\"\"\"\n",
    "\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 : <unk>\n2 : <start>\n3 : <end>\n4 : i\n5 : ,\n6 : the\n7 : you\n8 : and\n9 : a\n10 : to\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenizer에 구축된 단어 사전의 인덱스 확인\n",
    "\"\"\"\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### Source sentecne 와 Target sentence 분리\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[    2    79   622     9  4486     5 10323     5   646    20     9  3397\n     3     0]\n[   79   622     9  4486     5 10323     5   646    20     9  3397     3\n     0     0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RNN 학습을 위한 입력데이터 생성\n",
    "벡터화된 텐서데이터로부터 Source sentence 와 Tartget sentence 로 분리\n",
    "\"\"\"\n",
    "\n",
    "# tensor에서 마지막 토큰을 잘라내서 source sentence를 생성합니다. ( 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다 )\n",
    "src_input = tensor[:, :-1]    \n",
    "# tensor에서 <start>를 잘라내서 target sentence를 생성합니다.  \n",
    "tgt_input = tensor[:, 1:]       \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  2  33 261 ...   0   0   0]\n [  2  47   5 ...   0   0   0]\n [  2  79 300 ...   0   0   0]\n ...\n [  2   7  56 ...  76 457 136]\n [  2  44  17 ...   0   0   0]\n [  2   4  34 ...   0   0   0]]\nSource Train :  (124810, 14)\nTarget Train :  (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "sklearn으로 train셋과 test셋 분리 (모델 학습용)\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                            tgt_input, \n",
    "                                                            test_size=0.2,\n",
    "                                                            shuffle=True)\n",
    "print(enc_train)\n",
    "print(\"Source Train : \", enc_train.shape)\n",
    "print(\"Target Train : \", dec_train.shape)"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 데이터셋 객체 생성 <br><br>\n",
    "\n",
    "그동안 우리는 model.fit(x_train, y_train, …) 형태로 Numpy Array 데이터셋을 생성하여 model에 제공하는 형태의 학습을 많이 진행해 왔습니다. <br>\n",
    "그러나 텐서플로우를 활용할 경우 텐서로 생성된 데이터를 이용해 tf.data.Dataset객체를 생성하는 방법을 흔히 사용합니다. <br><br>\n",
    "\n",
    "__tf.data.Dataset객체__ <br>\n",
    "tf.data.Dataset객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의기능을 제공하므로 꼭 사용법을 알아 두시기를 권합니다. 우리는 이미 데이터셋을 텐서 형태로 생성해 두었으므로, tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset객체를 생성할 것입니다.\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "\"\"\"\n",
    "tf.data.Dataset 으로 학습을 위한 데이터셋 생성\n",
    "학습을 위한 하이퍼파라미터 설정\n",
    "\"\"\"\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "## 모델 학습\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 모델 구성 <br><br>\n",
    "\n",
    "우리가 만들 모델은 tf.keras.Model을 Subclassing하는 방식으로 만들 것입니다. 우리가 만들 모델에는 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있습니다. <br><br>\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "모델 생성\n",
    "enbedding layer 와 hidden layer 활용\n",
    "\"\"\"\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.55865710e-04,  5.37781489e-05,  1.24139740e-04, ...,\n",
       "         -9.72222188e-06, -9.23581829e-05, -1.56753813e-04],\n",
       "        [ 4.14280774e-04,  1.05145438e-04,  2.60636007e-04, ...,\n",
       "          2.22485560e-05, -4.36053990e-04, -1.82532487e-04],\n",
       "        [ 4.48071049e-04, -5.22301489e-05,  2.59812165e-04, ...,\n",
       "          2.49494100e-04, -6.51614740e-04,  1.44018923e-04],\n",
       "        ...,\n",
       "        [-1.34621761e-04, -6.27547270e-04,  6.45147287e-04, ...,\n",
       "         -3.60006758e-04, -4.43128956e-04,  1.71524985e-03],\n",
       "        [-3.85105785e-04, -7.15317205e-04,  6.17600221e-04, ...,\n",
       "         -3.78073019e-04, -3.26665468e-04,  1.71787036e-03],\n",
       "        [-5.00671507e-04, -5.24935022e-04,  5.40392590e-04, ...,\n",
       "         -4.42377874e-04, -2.02564246e-04,  1.92892039e-03]],\n",
       "\n",
       "       [[ 1.55865710e-04,  5.37781489e-05,  1.24139740e-04, ...,\n",
       "         -9.72222188e-06, -9.23581829e-05, -1.56753813e-04],\n",
       "        [ 1.18166237e-04,  3.38870304e-04,  8.61193184e-05, ...,\n",
       "         -9.27575893e-05,  1.13199647e-04, -4.09790658e-07],\n",
       "        [-1.65355159e-05,  6.65041560e-04, -4.94977830e-05, ...,\n",
       "         -1.53780187e-04,  4.56951675e-04,  2.76240287e-04],\n",
       "        ...,\n",
       "        [-8.16114247e-04, -3.21688014e-04, -6.88629807e-04, ...,\n",
       "          2.69568147e-04,  1.09141064e-03,  2.05822143e-04],\n",
       "        [-1.01375417e-03, -3.26824054e-04, -6.63433631e-04, ...,\n",
       "          2.57214910e-04,  1.36744359e-03,  2.14937652e-04],\n",
       "        [-1.18548353e-03, -3.15716403e-04, -6.78356620e-04, ...,\n",
       "          2.84478214e-04,  1.81359053e-03,  4.25041741e-04]],\n",
       "\n",
       "       [[ 1.55865710e-04,  5.37781489e-05,  1.24139740e-04, ...,\n",
       "         -9.72222188e-06, -9.23581829e-05, -1.56753813e-04],\n",
       "        [ 3.65857239e-04,  4.69641382e-05,  1.37862167e-04, ...,\n",
       "         -1.25045000e-04,  1.38533724e-04, -2.06156954e-04],\n",
       "        [ 1.90220962e-04,  2.28695746e-04,  1.26595391e-04, ...,\n",
       "         -3.04262649e-04,  6.53846946e-05, -3.89522611e-05],\n",
       "        ...,\n",
       "        [-1.62152573e-03, -8.45740433e-04, -2.29657031e-04, ...,\n",
       "          1.48556486e-04,  1.62432203e-03,  1.35642756e-03],\n",
       "        [-1.70182006e-03, -8.98299739e-04, -3.09032825e-04, ...,\n",
       "          2.01004965e-04,  2.24324153e-03,  1.58485526e-03],\n",
       "        [-1.75346015e-03, -9.33117757e-04, -4.02994599e-04, ...,\n",
       "          2.37813496e-04,  2.80232518e-03,  1.80549920e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.55865710e-04,  5.37781489e-05,  1.24139740e-04, ...,\n",
       "         -9.72222188e-06, -9.23581829e-05, -1.56753813e-04],\n",
       "        [ 2.18009242e-04,  2.44250317e-04,  2.11956300e-04, ...,\n",
       "          3.21715524e-05, -2.26746910e-04, -4.02855250e-04],\n",
       "        [ 2.71772413e-04,  4.07876796e-04,  1.09073851e-04, ...,\n",
       "         -1.42236546e-04, -5.47804928e-04, -5.62495319e-04],\n",
       "        ...,\n",
       "        [-1.17529300e-03,  1.53010973e-04, -5.01033966e-04, ...,\n",
       "         -7.88052057e-05,  2.06295960e-03,  1.04496395e-03],\n",
       "        [-1.32270053e-03,  3.77882607e-05, -5.84054796e-04, ...,\n",
       "         -2.73096030e-05,  2.68100901e-03,  1.41763838e-03],\n",
       "        [-1.44863327e-03, -7.79618640e-05, -6.79171702e-04, ...,\n",
       "          1.56220794e-05,  3.19858803e-03,  1.74290093e-03]],\n",
       "\n",
       "       [[ 1.55865710e-04,  5.37781489e-05,  1.24139740e-04, ...,\n",
       "         -9.72222188e-06, -9.23581829e-05, -1.56753813e-04],\n",
       "        [ 4.26790444e-04,  4.46389808e-04,  9.23687985e-05, ...,\n",
       "         -2.67182448e-04, -3.00707150e-04, -2.73436774e-04],\n",
       "        [ 1.41383221e-04,  6.50570553e-04, -1.17199656e-04, ...,\n",
       "         -3.88763816e-04, -4.03708364e-05, -7.07320869e-04],\n",
       "        ...,\n",
       "        [-3.51780909e-04, -1.32722419e-03, -8.52234225e-05, ...,\n",
       "          4.68733866e-04, -1.47496955e-03, -3.62688570e-06],\n",
       "        [-3.83972016e-04, -1.32960198e-03,  2.76746170e-04, ...,\n",
       "          2.26709410e-04, -1.31588418e-03,  2.59209686e-04],\n",
       "        [-6.38010679e-04, -1.23589241e-03,  5.05621545e-04, ...,\n",
       "          1.64289140e-05, -1.03616214e-03,  2.03009855e-04]],\n",
       "\n",
       "       [[ 1.55865710e-04,  5.37781489e-05,  1.24139740e-04, ...,\n",
       "         -9.72222188e-06, -9.23581829e-05, -1.56753813e-04],\n",
       "        [ 2.96543090e-04,  9.88009197e-05,  5.42169983e-05, ...,\n",
       "         -1.42283898e-04, -2.18934401e-05,  2.91452103e-04],\n",
       "        [-2.97649603e-05,  2.44144117e-04, -3.47483765e-05, ...,\n",
       "         -4.31636669e-04, -2.67912081e-04,  5.64264250e-04],\n",
       "        ...,\n",
       "        [-5.78988809e-04,  1.17585660e-04, -1.58157534e-04, ...,\n",
       "         -5.89518459e-06,  2.61177396e-04,  4.53176966e-04],\n",
       "        [-8.85887712e-04,  2.06660159e-04, -2.33642728e-04, ...,\n",
       "         -1.01796795e-04,  8.01551651e-05,  3.73654504e-04],\n",
       "        [-1.21887424e-03,  2.44351046e-04, -3.10704490e-04, ...,\n",
       "          1.27115141e-04, -4.99478519e-05,  5.68216085e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "\"\"\"\n",
    "모델 확인\n",
    "\n",
    "model은 아직 제대로 build되지 않았습니다.\n",
    "model.compile()을 호출한 적도 없고, 아직 model의 입력 텐서가 무엇인지 제대로 지정해 주지도 않았기 때문입니다.\n",
    "\n",
    "그런 경우 아래와 같이 model에 데이터를 아주 조금 태워 보는 것도 방법입니다.)\n",
    "model의 input shape가 결정되면서 model.build()가 자동으로 호출됩니다.\n",
    "\"\"\"\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break    # 하나의 배치만 데이터를 가져와 모델에 통과(?)\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"text_generator\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        multiple                  3072256   \n_________________________________________________________________\nlstm (LSTM)                  multiple                  5246976   \n_________________________________________________________________\nlstm_1 (LSTM)                multiple                  8392704   \n_________________________________________________________________\ndense (Dense)                multiple                  12301025  \n=================================================================\nTotal params: 29,012,961\nTrainable params: 29,012,961\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "모델 구성 출력\n",
    "\"\"\"\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 모델 학습 <br><br>\n",
    "\n",
    "tensorflow를 통한 모델학습은 model.compile() 후에 model.fit()을 진행해야 합니다.\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "609/609 [==============================] - 82s 134ms/step - loss: 3.4220\n",
      "Epoch 2/10\n",
      "609/609 [==============================] - 85s 139ms/step - loss: 2.9546\n",
      "Epoch 3/10\n",
      "609/609 [==============================] - 85s 139ms/step - loss: 2.7811\n",
      "Epoch 4/10\n",
      "609/609 [==============================] - 83s 137ms/step - loss: 2.6520\n",
      "Epoch 5/10\n",
      "609/609 [==============================] - 84s 138ms/step - loss: 2.5422\n",
      "Epoch 6/10\n",
      "609/609 [==============================] - 85s 140ms/step - loss: 2.4430\n",
      "Epoch 7/10\n",
      "609/609 [==============================] - 86s 142ms/step - loss: 2.3518\n",
      "Epoch 8/10\n",
      "609/609 [==============================] - 86s 142ms/step - loss: 2.2657\n",
      "Epoch 9/10\n",
      "609/609 [==============================] - 86s 142ms/step - loss: 2.1852\n",
      "Epoch 10/10\n",
      "609/609 [==============================] - 84s 138ms/step - loss: 2.1085\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6ae970c490>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)       # 10 epoch 안에 val_loss 2.2 수준으로 만들기"
   ]
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 모델 평가하기\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "작문 함수 작성\n",
    "작문 모델을 평가하는 가장 확실한 방법은 작문을 시켜보고 직접 평가하는 겁니다. \n",
    "아래 generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 합니다.\n",
    "\"\"\"\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "\"\"\"\n",
    "문장생성 함수로 문장 생성\n",
    "\"\"\"\n",
    "\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  }
 ]
}